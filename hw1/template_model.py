# -*- coding: utf-8 -*-
"""hw1_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tCz5nCDpFpnDYeJaxpGLt49XUo3zJnLq
"""

import os
import torch
import torch.nn as nn
import numpy as np
from torch.utils import data
from torch.utils.data import DataLoader
from torchvision.datasets import DatasetFolder
from torchvision.transforms import transforms
import torchvision.models as models
from PIL import Image
import glob
import argparse
from tqdm.auto import tqdm
# !unzip -q "/content/drive/MyDrive/DLCV/hw1/hw1_data.zip" -d "/content/drive/MyDrive/DLCV/hw1/hw1_data"

# !python "/content/drive/MyDrive/DLCV/hw1/test_ipy.ipynb"
# import os
# print(os.path.exists("/content/drive/MyDrive/DLCV/hw1/try.py"))

from google.colab import drive
drive.mount('/content/drive')

# import glob 
from sklearn.model_selection import train_test_split
train_list = glob.glob(os.path.join("/content/drive/MyDrive/DLCV/hw1/hw1_data/p1_data/train_50", "*.png"))
test_list = glob.glob(os.path.join("/content/drive/MyDrive/DLCV/hw1/hw1_data/p1_data/val_50", "*.png"))
# print(train_list)
train_list, val_list = train_test_split(train_list, train_size = 0.8, random_state = 2022)
train_list, val_list = np.array(train_list), np.array(val_list) 
# print(train_list[0])

class Dataset(data.Dataset):
  def __init__(self, img_list, mode, transform = None):
    self.img_list = img_list
    self.transform = transform
    self.mode = mode

  def __getitem__(self, idx):
    img_path = self.img_list[idx]
    original_img = Image.open(img_path)
    transformed_img = self.transform(original_img)
    label = 0
    if(self.mode == "test"):
      label = 0
    else:
      label = int(img_path.split("/")[-1].split("_")[0])

    return transformed_img, label

  def __len__(self):
    return len(self.img_list)

train_transform = transforms.Compose([
    transforms.RandomResizedCrop((128, 128)),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

test_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

valid_transform = transforms.Compose([
    transforms.Resize((128, 128)),
    transforms.ToTensor(),
    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])
train_imgs = Dataset(train_list, transform = train_transform, mode = "train")
val_imgs = Dataset(val_list, transform = valid_transform, mode = "valid")
test_imgs = Dataset(test_list, transform = test_transform, mode = "test")

train_loader = DataLoader(dataset = train_imgs, batch_size = 15, shuffle = True)
val_loader = DataLoader(dataset = val_imgs, batch_size = 15, shuffle = True)
test_loader = DataLoader(dataset = test_imgs, batch_size = 15, shuffle = True)

for imgs, batch in train_loader:
  print(imgs.size())
  # obj1, obj2 = batch
  print(batch)
  break

# class VGG16(nn.Module):
#   def __init__(self, num_classes = 50):
#     super(VGG16, self).__init__()
#     self.layer1 = nn.Sequential(
#         nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(64),
#         nn.ReLU()
#     ) 
#     self.layer2 = nn.Sequential(
#         nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(64),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2)
#     )
#     self.layer3 = nn.Sequential(
#         nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(128),
#         nn.ReLU()
#     )
#     self.layer4 = nn.Sequential(
#         nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(128),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.layer5 = nn.Sequential(
#         nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(256),
#         nn.ReLU()
#     )
#     self.layer6 = nn.Sequential(
#         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(256),
#         nn.ReLU()
        
#     )
#     self.layer7 = nn.Sequential(
#         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(256),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.layer8 = nn.Sequential(
#         nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer9 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer10 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.layer11 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer12 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer13 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.fc = nn.Sequential(
#         nn.Dropout(0.5),
#         nn.Linear(7*7*512, 4096),
#         nn.ReLU()
#     )
#     self.fc1 = nn.Sequential(
#         nn.Dropout(0.5),
#         nn.Linear(4096, 4096),
#         nn.ReLU()
#     )
#     self.fc2= nn.Sequential(
#         nn.Linear(4096, num_classes)
#     )
#   def forward(self, x):
#     out = self.layer1(x)
#     out = self.layer2(out)
#     out = self.layer3(out)
#     out = self.layer4(out)
#     out = self.layer5(out)
#     out = self.layer6(out)
#     out = self.layer7(out)
#     out = self.layer8(out)
#     out = self.layer9(out)
#     out = self.layer10(out)
#     out = self.layer11(out)
#     out = self.layer12(out)
#     out = self.layer13(out)
#     out = out.reshape(out.size(0), -1)
#     out = self.fc(out)
#     out = self.fc1(out)
#     out = self.fc2(out)
#     return out


# class VGG16(nn.Module):
#   def __init__(self, num_classes = 50):
#     super(VGG16, self).__init__()
#     net = models.vgg16(pretrained = False)
#     net.classifier = nn.Sequential()
#     self.features = net
#     self.classifier = nn.Sequential(
#         nn.Linear(512*7*7, 512),
#         nn.ReLU(True),
#         nn.Dropout(),
#         nn.Linear(512, 128),
#         nn.ReLU(True),
#         nn.Dropout(),
#         nn.Linear(128, num_classes)
#     )

#   def forward(self, x):
#     x = self.features(x),
#     x = x.view(x.size(0), -1),
#     x = self.classifier(x)
#     return x
# def conv_layer(chann_in, chann_out, k_size, p_size):
#     layer = nn.Sequential(
#         nn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),
#         nn.BatchNorm2d(chann_out),
#         nn.ReLU()
#     )
#     return layer

# def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):

#     layers = [ conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list)) ]
#     layers += [ nn.MaxPool2d(kernel_size = pooling_k, stride = pooling_s)]
#     return nn.Sequential(*layers)

# def vgg_fc_layer(size_in, size_out):
#     layer = nn.Sequential(
#         nn.Linear(size_in, size_out),
#         nn.BatchNorm1d(size_out),
#         nn.ReLU()
#     )
#     return layer

# class VGG16(nn.Module):
#     def __init__(self, n_classes=50):
#         super(VGG16, self).__init__()

#         # Conv blocks (BatchNorm + ReLU activation added in each block)
#         self.layer1 = vgg_conv_block([3,64], [64,64], [3,3], [1,1], 2, 2)
#         self.layer2 = vgg_conv_block([64,128], [128,128], [3,3], [1,1], 2, 2)
#         self.layer3 = vgg_conv_block([128,256,256], [256,256,256], [3,3,3], [1,1,1], 2, 2)
#         self.layer4 = vgg_conv_block([256,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)
#         self.layer5 = vgg_conv_block([512,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)

#         # FC layers
#         self.layer6 = vgg_fc_layer(7*7*512, 4096)
#         self.layer7 = vgg_fc_layer(4096, 4096)

#         # Final layer
#         self.layer8 = nn.Linear(4096, n_classes)

#     def forward(self, x):
#         out = self.layer1(x)
#         out = self.layer2(out)
#         out = self.layer3(out)
#         out = self.layer4(out)
#         vgg16_features = self.layer5(out)
#         out = vgg16_features.view(out.size(0), -1)
#         out = self.layer6(out)
#         out = self.layer7(out)
#         out = self.layer8(out)

#         return out


class VGG16(nn.Module):
    def __init__(self):
        super(VGG16, self).__init__()
        # The arguments for commonly used modules:
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)

        # input image size: [3, 128, 128]
        self.cnn_layers = nn.Sequential(
            models.densenet161(pretrained=False)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(1000, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 50)
        )

    def forward(self, x):
        # input (x): [batch_size, 3, 128, 128]
        # output: [batch_size, 11]

        # Extract features by convolutional layers.
        x = self.cnn_layers(x)
        # The extracted feature map must be flatten before going to fully-connected layers.
        x = x.flatten(1)
        # print(x.shape)

        # The features are transformed by fully-connected layers to obtain the final logits.
        x = self.fc_layers(x)
        return x

device = "cuda" if torch.cuda.is_available() else "cpu"
model = VGG16().to(device)
model.device = device
prev_epoch = 0

# load model if exists
if(os.stat("/content/drive/MyDrive/DLCV/hw1/best_model.pt").st_size != 0):
  print("model exists")
  checkpoint = None
  if(torch.cuda.is_available()):
    checkpoint = torch.load("/content/drive/MyDrive/DLCV/hw1/best_model.pt")
  else:
    checkpoint = torch.load("/content/drive/MyDrive/DLCV/hw1/best_model.pt", map_location = "cpu")
  model.load_state_dict(checkpoint["model_state_dict"])
  prev_epoch = checkpoint["epoch"]

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)
n_epochs = 100

max_valid_acc = 0

for epoch in range(prev_epoch, n_epochs):
  model.train()
  train_losses = []
  train_accs = []

  for batch in tqdm(train_loader):
    imgs, labels = batch
    # labels = torch.tensor(labels)
    # print("size = ", imgs.to(device).size())
    logits = model(imgs.to(device))
    loss = criterion(logits, labels.to(device))
    optimizer.zero_grad()
    loss.backward()
    grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
    optimizer.step()

    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

    train_losses.append(loss.item())
    train_accs.append(acc)
  
  train_loss = sum(train_losses) / len(train_losses)
  train_acc = sum(train_accs) / len(train_accs)

  print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")
  

  model.eval()

  valid_losses = []
  valid_accs = []

  for batch in tqdm(val_loader):
    imgs, labels = batch

    with torch.no_grad():
      logits = model(imgs.to(device))
    loss = criterion(logits, labels.to(device))
    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

    valid_losses.append(loss.item())
    valid_accs.append(acc)

  valid_loss = sum(valid_losses) / len(valid_losses)
  valid_acc = sum(valid_accs) / len(valid_accs)
  
  print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")

  if(valid_acc > max_valid_acc):
    torch.save({"model_state_dict":model.state_dict(),
          "epoch":epoch
          }, "/content/drive/MyDrive/DLCV/hw1/best_model.pt")
    max_valid_acc = valid_acc
    with open("/content/drive/MyDrive/DLCV/hw1/record.txt", "a") as f:
      f.write(f"saving model\n")
  with open("/content/drive/MyDrive/DLCV/hw1/record.txt", "a") as f:
    f.write(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\n")
    f.write(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\n")
    f.write(f"max_valid_acc = {max_valid_acc:.5f}\n")