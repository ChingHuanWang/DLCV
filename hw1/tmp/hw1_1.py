# -*- coding: utf-8 -*-
"""hw1_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tCz5nCDpFpnDYeJaxpGLt49XUo3zJnLq
"""

import os
import torch
import torch.nn as nn
import numpy as np
from torch.utils import data
from torch.utils.data import DataLoader
from torchvision.datasets import DatasetFolder
from torchvision.transforms import transforms
import torchvision.models as models
from PIL import Image
import glob
import argparse
from tqdm.auto import tqdm
from sklearn.model_selection import train_test_split
import random


simple_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    transforms.Resize((96, 96))
])

crop_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    transforms.CenterCrop((20, 20)),
    transforms.Resize((96, 96))
])

rot_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    transforms.Resize((96, 96)),
    transforms.RandomRotation((-30, 30)),
    transforms.RandomHorizontalFlip(p = 0.5)
])

noise_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    transforms.Resize((96, 96)),
    transforms.GaussianBlur(kernel_size=(3, 3), sigma=(10, 10))
])

val_transform = transforms.Compose([
    transforms.Resize((96, 96)),
    transforms.ToTensor()
])

test_transform = transforms.Compose([
    transforms.Resize((96, 96)),
    transforms.ToTensor()
])

transform_set = [
    simple_transform,
    crop_transform,
    rot_transform,
    noise_transform,
    val_transform,
    test_transform
]

# read original image
training_img_path_list = glob.glob(os.path.join("./p1_data/train_50", "*.png"))
testing_img_path_list = glob.glob(os.path.join("./p1_data/val_50", "*.png"))
training_img_path_list = [[img_path, 0] for img_path in training_img_path_list]
training_label_list = np.array([int(img_path[0].split("/")[-1].split("_")[0]) for img_path in training_img_path_list])


# training_img_list = np.array([Image.open(img_path) for img_path in training_img_path_list])
# testing_img_list = np.array([Image.open(img_path) for img_path in testing_img_path_list])



# split data to 8:2 and do image transformation
training_img_path_list, val_img_path_list, training_label_list, val_label_list = train_test_split(training_img_path_list, training_label_list, train_size = 0.8, random_state = 2022)

def get_sorted_img_and_label_list(img_path_list, label_list):
    sorted_idxs = np.argsort(label_list)
    label_list = np.array(label_list)[sorted_idxs]
    img_path_list = np.array(img_path_list)[sorted_idxs]

    return img_path_list, label_list


def get_augmented_img_and_label_list(img_path_list, label_list):
    random_nums = np.sort(np.array(random.sample(range(360), 90)))
    img_path_list, label_list = get_sorted_img_and_label_list(img_path_list, label_list)
    transform_list = np.zeros(len(label_list))

    extra_img_path_list = []
    extra_label_list = []
    extra_transform_list = []

    for i in range(0, len(label_list), 360):
        for idx, num in enumerate(random_nums):
            extra_img_path_list.append([img_path_list[i+num][0], int(idx/30)+1])
            extra_label_list.append(label_list[i+num])

    # for i in range(0, 90, 30):
    #     for j in range(i, i+30):
    #         extra_img_path_list[j] = transform_set[i/30+1](extra_img_path_list[j])

    

    augmented_img_path_list = np.concatenate((img_path_list, np.array(extra_img_path_list)), axis = 0)
    augmented_label_list = np.concatenate((label_list, np.array(extra_label_list)), axis = None)

    return augmented_img_path_list, augmented_label_list

training_img_path_list, training_label_list = get_augmented_img_and_label_list(training_img_path_list, training_label_list)
training_img_path_list, training_label_list = np.array(training_img_path_list), np.array(training_label_list)
val_img_path_list, val_label_list = np.array([[img_path[0], 4] for img_path in val_img_path_list]), np.array(val_label_list)
testing_img_path_list = np.array([[img_path, 5] for img_path in testing_img_path_list])

print(training_img_path_list)
print(training_label_list)
print(val_img_path_list)
print(val_label_list)

print(testing_img_path_list)
# print(test_label_list)
# print(train_list[0])

class Dataset(data.Dataset):
    def __init__(self, img_list, label_list, transform_set, mode):
        self.img_list = img_list
        self.label_list = label_list
        self.mode = mode

    def __getitem__(self, idx):
        img = Image.open(self.img_list[idx][0])
        # print("val = ", self.img_list[idx][0])
        transformed_img = transform_set[int(self.img_list[idx][1])](img)
        label = 0
        if(self.mode == "test"):
            label = 0
        else:
            label = self.label_list[idx]
        return transformed_img, label

    def __len__(self):
        return len(self.img_list)


training_data = Dataset(training_img_path_list, training_label_list, transform_set, mode = "train")
val_data = Dataset(val_img_path_list, val_label_list, transform_set, mode = "valid")
testing_data = Dataset(testing_img_path_list, None, transform_set, mode = "test")

training_loader = DataLoader(dataset = training_data, batch_size = 15, shuffle = True)
val_loader = DataLoader(dataset = val_data, batch_size = 15, shuffle = True)
testing_loader = DataLoader(dataset = testing_data, batch_size = 15, shuffle = True)



# class VGG16(nn.Module):
#   def __init__(self, num_classes = 50):
#     super(VGG16, self).__init__()
#     self.layer1 = nn.Sequential(
#         nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(64),
#         nn.ReLU()
#     ) 
#     self.layer2 = nn.Sequential(
#         nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(64),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size=2, stride=2)
#     )
#     self.layer3 = nn.Sequential(
#         nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(128),
#         nn.ReLU()
#     )
#     self.layer4 = nn.Sequential(
#         nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(128),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.layer5 = nn.Sequential(
#         nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(256),
#         nn.ReLU()
#     )
#     self.layer6 = nn.Sequential(
#         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(256),
#         nn.ReLU()
        
#     )
#     self.layer7 = nn.Sequential(
#         nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(256),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.layer8 = nn.Sequential(
#         nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer9 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer10 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.layer11 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer12 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU()
#     )
#     self.layer13 = nn.Sequential(
#         nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),
#         nn.BatchNorm2d(512),
#         nn.ReLU(),
#         nn.MaxPool2d(kernel_size = 2, stride = 2)
#     )
#     self.fc = nn.Sequential(
#         nn.Dropout(0.5),
#         nn.Linear(7*7*512, 4096),
#         nn.ReLU()
#     )
#     self.fc1 = nn.Sequential(
#         nn.Dropout(0.5),
#         nn.Linear(4096, 4096),
#         nn.ReLU()
#     )
#     self.fc2= nn.Sequential(
#         nn.Linear(4096, num_classes)
#     )
#   def forward(self, x):
#     out = self.layer1(x)
#     out = self.layer2(out)
#     out = self.layer3(out)
#     out = self.layer4(out)
#     out = self.layer5(out)
#     out = self.layer6(out)
#     out = self.layer7(out)
#     out = self.layer8(out)
#     out = self.layer9(out)
#     out = self.layer10(out)
#     out = self.layer11(out)
#     out = self.layer12(out)
#     out = self.layer13(out)
#     out = out.reshape(out.size(0), -1)
#     out = self.fc(out)
#     out = self.fc1(out)
#     out = self.fc2(out)
#     return out


# class VGG16(nn.Module):
#   def __init__(self, num_classes = 50):
#     super(VGG16, self).__init__()
#     net = models.vgg16(pretrained = False)
#     net.classifier = nn.Sequential()
#     self.features = net
#     self.classifier = nn.Sequential(
#         nn.Linear(512*7*7, 512),
#         nn.ReLU(True),
#         nn.Dropout(),
#         nn.Linear(512, 128),
#         nn.ReLU(True),
#         nn.Dropout(),
#         nn.Linear(128, num_classes)
#     )

#   def forward(self, x):
#     x = self.features(x),
#     x = x.view(x.size(0), -1),
#     x = self.classifier(x)
#     return x
# def conv_layer(chann_in, chann_out, k_size, p_size):
#     layer = nn.Sequential(
#         nn.Conv2d(chann_in, chann_out, kernel_size=k_size, padding=p_size),
#         nn.BatchNorm2d(chann_out),
#         nn.ReLU()
#     )
#     return layer

# def vgg_conv_block(in_list, out_list, k_list, p_list, pooling_k, pooling_s):

#     layers = [ conv_layer(in_list[i], out_list[i], k_list[i], p_list[i]) for i in range(len(in_list)) ]
#     layers += [ nn.MaxPool2d(kernel_size = pooling_k, stride = pooling_s)]
#     return nn.Sequential(*layers)

# def vgg_fc_layer(size_in, size_out):
#     layer = nn.Sequential(
#         nn.Linear(size_in, size_out),
#         nn.BatchNorm1d(size_out),
#         nn.ReLU()
#     )
#     return layer

# class VGG16(nn.Module):
#     def __init__(self, n_classes=50):
#         super(VGG16, self).__init__()

#         # Conv blocks (BatchNorm + ReLU activation added in each block)
#         self.layer1 = vgg_conv_block([3,64], [64,64], [3,3], [1,1], 2, 2)
#         self.layer2 = vgg_conv_block([64,128], [128,128], [3,3], [1,1], 2, 2)
#         self.layer3 = vgg_conv_block([128,256,256], [256,256,256], [3,3,3], [1,1,1], 2, 2)
#         self.layer4 = vgg_conv_block([256,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)
#         self.layer5 = vgg_conv_block([512,512,512], [512,512,512], [3,3,3], [1,1,1], 2, 2)

#         # FC layers
#         self.layer6 = vgg_fc_layer(7*7*512, 4096)
#         self.layer7 = vgg_fc_layer(4096, 4096)

#         # Final layer
#         self.layer8 = nn.Linear(4096, n_classes)

#     def forward(self, x):
#         out = self.layer1(x)
#         out = self.layer2(out)
#         out = self.layer3(out)
#         out = self.layer4(out)
#         vgg16_features = self.layer5(out)
#         out = vgg16_features.view(out.size(0), -1)
#         out = self.layer6(out)
#         out = self.layer7(out)
#         out = self.layer8(out)

#         return out


class Densenet161(nn.Module):
    def __init__(self):
        super(Densenet161, self).__init__()
        # The arguments for commonly used modules:
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)

        # input image size: [3, 128, 128]
        self.cnn_layers = nn.Sequential(
            models.densenet161(pretrained=False)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(1000, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 50)
        )

    def forward(self, x):
        # input (x): [batch_size, 3, 128, 128]
        # output: [batch_size, 11]

        # Extract features by convolutional layers.
        x = self.cnn_layers(x)
        # The extracted feature map must be flatten before going to fully-connected layers.
        x = x.flatten(1)
        # print(x.shape)

        # The features are transformed by fully-connected layers to obtain the final logits.
        x = self.fc_layers(x)
        return x

device = "cuda" if torch.cuda.is_available() else "cpu"
model = Densenet161().to(device)
model.device = device
prev_epoch = 0
max_valid_acc = 0

# load model if exists
if(os.stat("best_model.pt").st_size != 0):
    print("model exists")
    checkpoint = None
    if(torch.cuda.is_available()):
        checkpoint = torch.load("best_model.pt")
    else:
        checkpoint = torch.load("best_model.pt", map_location = "cpu")
        
    max_valid_acc = checkpoint["max_valid_acc"]
    model.load_state_dict(checkpoint["model_state_dict"])
    prev_epoch = checkpoint["epoch"]

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)
n_epochs = 200



# training model

for epoch in range(prev_epoch, n_epochs):
    model.train()
    train_losses = []
    train_accs = []

    for batch in tqdm(training_loader):
        imgs, labels = batch
        # labels = torch.tensor(labels)
        # print("size = ", imgs.to(device).size())
        logits = model(imgs.to(device))
        loss = criterion(logits, labels.to(device))
        optimizer.zero_grad()
        loss.backward()
        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
        optimizer.step()

        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        train_losses.append(loss.item())
        train_accs.append(acc)
  
    train_loss = sum(train_losses) / len(train_losses)
    train_acc = sum(train_accs) / len(train_accs)

    print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")


    model.eval()

    valid_losses = []
    valid_accs = []

    for batch in tqdm(val_loader):
        imgs, labels = batch

        with torch.no_grad():
            logits = model(imgs.to(device))
        loss = criterion(logits, labels.to(device))
        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

        valid_losses.append(loss.item())
        valid_accs.append(acc)

    valid_loss = sum(valid_losses) / len(valid_losses)
    valid_acc = sum(valid_accs) / len(valid_accs)

    print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")

    if(valid_acc > max_valid_acc):
        torch.save({"model_state_dict":model.state_dict(),
                    "epoch":epoch,
                    "max_valid_acc":max_valid_acc

        }, "best_model.pt")
        max_valid_acc = valid_acc
        with open("record.txt", "a") as f:
            f.write(f"saving model\n")
  
    with open("record.txt", "a") as f:
        f.write(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\n")
        f.write(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\n")
        f.write(f"max_valid_acc = {max_valid_acc:.5f}\n")


# testing model

model.eval()
predictions = []

for batch in tqdm(testing_loader):

    imgs, labels = batch
    with torch.no_grad():
        logits = model(imgs.to(device))

    predictions.extend(logits.argmax(dim=-1).cpu().numpy().tolist())

# save prediction to file

with open("prediction.csv", "w") as f:

    f.write("Id, Category\n")

    for i, pred in enumerate(predictions):
        f.write(f"{i}, {pred}\n")