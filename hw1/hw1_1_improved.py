# -*- coding: utf-8 -*-
"""hw1_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tCz5nCDpFpnDYeJaxpGLt49XUo3zJnLq
"""

import os
import torch
import torch.nn as nn
import numpy as np
from torch.utils import data
from torch.utils.data import DataLoader
from torchvision.datasets import DatasetFolder
from torchvision.transforms import transforms
import torchvision.models as models
from PIL import Image
import glob
import argparse
from tqdm.auto import tqdm 
from sklearn.model_selection import train_test_split


train_list = glob.glob(os.path.join("/content/drive/MyDrive/DLCV/hw1/hw1_data/p1_data/train_50", "*.png"))
test_list = glob.glob(os.path.join("/content/drive/MyDrive/DLCV/hw1/hw1_data/p1_data/val_50", "*.png"))
train_list, val_list = train_test_split(train_list, train_size = 0.8, random_state = 2022)
train_list, val_list = np.array(train_list), np.array(val_list) 


class Dataset(data.Dataset):
  def __init__(self, img_list, transform = None, random_transform = random_transform, mode):
    self.img_list = img_list
    self.transform = transform
    self.random_transform = random_transform
    self.mode = mode

  def __getitem__(self, idx):
    img_path = self.img_list[idx]
    original_img = Image.open(img_path)
    transformed_img = self.transform(original_img)
    label = 0
    if(self.mode == "test"):
      label = 0
    else:
      label = int(img_path.split("/")[-1].split("_")[0])
      if(self.mode == "train"):
        transformed_img = self.random_transform(transformed_img)

    return transformed_img, label

  def __len__(self):
    return len(self.img_list)

train_transform = transforms.Compose([
  transforms.Resize((32, 32)),
  transforms.ToTensor(),
  transforms.Normalize((0.5077, 0.4813, 0.4312), (0.2000, 0.1986, 0.2034)),
  transforms.RandomHorizontalFlip(p=0.25)
  transforms.RandomRotation((-30, 30)),
  # transforms.GaussianBlur(kernel_size = (3, 3), sigma = (5, 5)),
  transforms.RandomInvert(p=0.25),
  transforms.RandomGrayscale(p=0.25),
  transforms.RandomAdjustSharpness(10, p =0.25),
  transforms.ColorJitter(brightness = (0.5, 1.5), contrast = (0.5, 1.5), saturation = (0.5, 1.5), hue = (-0.1, 0.1))
])

test_transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

valid_transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.ToTensor(),
    # transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

random_transform = transforms.GaussianBlur(kernel_size = (3, 3), sigma = (5, 5))


train_imgs = Dataset(train_list, transform = train_transform, random_transform = random_transform, mode = "train")
val_imgs = Dataset(val_list, transform = valid_transform, random_transform = None, mode = "valid")
test_imgs = Dataset(test_list, transform = test_transform, random_transform = None, mode = "test")

train_loader = DataLoader(dataset = train_imgs, batch_size = 15, shuffle = True)
val_loader = DataLoader(dataset = val_imgs, batch_size = 15, shuffle = True)
test_loader = DataLoader(dataset = test_imgs, batch_size = 15, shuffle = True)

model_path = 


class VGG16(nn.Module):
    def __init__(self):
        super(VGG16, self).__init__()
        # The arguments for commonly used modules:
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)

        # input image size: [3, 128, 128]
        self.cnn_layers = nn.Sequential(
            models.densenet161(pretrained=False)
        )
        self.fc_layers = nn.Sequential(
            nn.Linear(1000, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 50)
        )

    def forward(self, x):
        # input (x): [batch_size, 3, 128, 128]
        # output: [batch_size, 11]

        # Extract features by convolutional layers.
        x = self.cnn_layers(x)
        # The extracted feature map must be flatten before going to fully-connected layers.
        x = x.flatten(1)
        # print(x.shape)

        # The features are transformed by fully-connected layers to obtain the final logits.
        x = self.fc_layers(x)
        return x

device = "cuda" if torch.cuda.is_available() else "cpu"
model = VGG16().to(device)
model.device = device
prev_epoch = 0

# load model if exists
if(os.stat("/content/drive/MyDrive/DLCV/hw1/best_model.pt").st_size != 0):
  print("model exists")
  checkpoint = None
  if(torch.cuda.is_available()):
    checkpoint = torch.load("/content/drive/MyDrive/DLCV/hw1/best_model.pt")
  else:
    checkpoint = torch.load("/content/drive/MyDrive/DLCV/hw1/best_model.pt", map_location = "cpu")
  model.load_state_dict(checkpoint["model_state_dict"])
  prev_epoch = checkpoint["epoch"]

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)
n_epochs = 100

max_valid_acc = 0

for epoch in range(prev_epoch, n_epochs):
  model.train()
  train_losses = []
  train_accs = []

  for batch in tqdm(train_loader):
    imgs, labels = batch
    # labels = torch.tensor(labels)
    # print("size = ", imgs.to(device).size())
    logits = model(imgs.to(device))
    loss = criterion(logits, labels.to(device))
    optimizer.zero_grad()
    loss.backward()
    grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
    optimizer.step()

    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

    train_losses.append(loss.item())
    train_accs.append(acc)
  
  train_loss = sum(train_losses) / len(train_losses)
  train_acc = sum(train_accs) / len(train_accs)

  print(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}")
  

  model.eval()

  valid_losses = []
  valid_accs = []

  for batch in tqdm(val_loader):
    imgs, labels = batch

    with torch.no_grad():
      logits = model(imgs.to(device))
    loss = criterion(logits, labels.to(device))
    acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()

    valid_losses.append(loss.item())
    valid_accs.append(acc)

  valid_loss = sum(valid_losses) / len(valid_losses)
  valid_acc = sum(valid_accs) / len(valid_accs)
  
  print(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}")

  if(valid_acc > max_valid_acc):
    torch.save({"model_state_dict":model.state_dict(),
          "epoch":epoch
          }, "/content/drive/MyDrive/DLCV/hw1/best_model.pt")
    max_valid_acc = valid_acc
    with open("/content/drive/MyDrive/DLCV/hw1/record.txt", "a") as f:
      f.write(f"saving model\n")
  with open("/content/drive/MyDrive/DLCV/hw1/record.txt", "a") as f:
    f.write(f"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\n")
    f.write(f"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\n")
    f.write(f"max_valid_acc = {max_valid_acc:.5f}\n")